{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46215a4e",
   "metadata": {},
   "source": [
    "## File Upload Testing in Azure OpenAI (AI Foundry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8d092",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512b7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file batch configuration\n",
    "TOTAL_FILES = 200\n",
    "FILE_SIZE_KB = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5bdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables for Azure OpenAI\n",
    "AOAI_API_BASE = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "AOAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AOAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_API_DEPLOY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6ff3b",
   "metadata": {},
   "source": [
    "### Azure OpenAI Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d629ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise token provider\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d7f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Azure OpenAI client\n",
    "client = AzureOpenAI(  \n",
    "    azure_endpoint = AOAI_API_BASE,\n",
    "    azure_ad_token_provider = token_provider,\n",
    "    api_version = AOAI_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0cf501",
   "metadata": {},
   "source": [
    "### File Batch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3decfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test files\n",
    "def create_test_files():\n",
    "    # Create test files in current directory subdirectory\n",
    "    test_dir = os.path.join(os.getcwd(), \"test_files\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if os.path.exists(test_dir):\n",
    "        shutil.rmtree(test_dir)\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "    files = []\n",
    "    content_base = \"This is test content for vector store batch upload testing. \" * (FILE_SIZE_KB * 15)\n",
    "    \n",
    "    print(f\"Creating test files in...\")\n",
    "    for i in range(TOTAL_FILES):\n",
    "        file_path = os.path.join(test_dir, f\"test_doc_{i:03d}.txt\")\n",
    "        content = f\"Document {i+1}\\n{content_base}\\nEnd of document {i+1}\\nTimestamp: {time.time()}\"\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        files.append(file_path)\n",
    "    \n",
    "    print(f\"Created {TOTAL_FILES} test files\")\n",
    "    return files, test_dir\n",
    "\n",
    "test_files, temp_dir = create_test_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a852dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store: vs_uB9u1bj3FblzSNsMxbUKzwEi\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "def create_vector_store():\n",
    "    vector_store = client.vector_stores.create(\n",
    "        name=f\"Batch Test Vector Store - {int(time.time())}\"\n",
    "    )\n",
    "    print(f\"Created vector store: {vector_store.id}\")\n",
    "    return vector_store.id\n",
    "\n",
    "vector_store_id = create_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f0162",
   "metadata": {},
   "source": [
    "### Option 1: Individual Upload of Files to Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ddbb4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 50 files individually to OpenAI...\n",
      "  Uploaded 1/50: assistant-31psNk2GF9gGb5CxLLBbGm\n",
      "  Uploaded 2/50: assistant-CgtUGQ4uQeEdrJmbxDYBcx\n",
      "  Uploaded 3/50: assistant-DhddtZi9icff3GcDp6BHtk\n",
      "  Uploaded 4/50: assistant-NAZb2Dq1Qk1pViA1GKop6g\n",
      "  Uploaded 5/50: assistant-Ryg7X5uNaztaNaB7eMEFju\n",
      "  Uploaded 6/50: assistant-6mQfZccYc2mBgUKcGX5umu\n",
      "  Uploaded 7/50: assistant-WfgdaX8Sdghrqqm2qtAjVW\n",
      "  Uploaded 8/50: assistant-UEsw4UFjz1QTEfGeEtzqXt\n",
      "  Uploaded 9/50: assistant-HWmwVcrqzQdoZejvJqt8fH\n",
      "  Uploaded 10/50: assistant-P6JE5oKdhZJEmLh983JPFH\n",
      "  Uploaded 11/50: assistant-83MtnFYXYnfrJCRJqWmwxH\n",
      "  Uploaded 12/50: assistant-Md4eBZveZnA7FG27odCtkY\n",
      "  Uploaded 13/50: assistant-7Fww5FtDayAPJZRUtC1p9c\n",
      "  Uploaded 14/50: assistant-6mwWuH5TAnL2zWxd1f9Ckg\n",
      "  Uploaded 15/50: assistant-EZuzoxBihWRmdZqAKQon7X\n",
      "  Uploaded 16/50: assistant-Pxyjw8QdfeEMAFc4KaRZS9\n",
      "  Uploaded 17/50: assistant-J7xr4JdNKhaXB7nx1wMN4q\n",
      "  Uploaded 18/50: assistant-9czfE5aB43wp3apME8B3sP\n",
      "  Uploaded 19/50: assistant-QMBseBoUQuBUPq7zW9nMk5\n",
      "  Uploaded 20/50: assistant-Kxo4rT75HHHcSSJAFymdkK\n",
      "  Uploaded 21/50: assistant-SjwGHnhHVHicd5SPVdeBqh\n",
      "  Uploaded 22/50: assistant-SVzCSJ2X5X64V85wdrqFqB\n",
      "  Uploaded 23/50: assistant-7TnBdykAogFqjmiboqjygz\n",
      "  Uploaded 24/50: assistant-43orgQBfeLbX5qGvY4E42R\n",
      "  Uploaded 25/50: assistant-DGpog2fEiY2abUW9zWn1j1\n",
      "  Uploaded 26/50: assistant-PWi2cY9LKcTayCW2VsSyEq\n",
      "  Uploaded 27/50: assistant-Jd7jjT9DnMmQNj98G5pDTM\n",
      "  Uploaded 28/50: assistant-Ka2viKs8knFbKpsZLfxkJM\n",
      "  Uploaded 29/50: assistant-EEPw3L7GKV8iSJPEzDSRVU\n",
      "  Uploaded 30/50: assistant-7HnDFKrov576iMZNzeP5ca\n",
      "  Uploaded 31/50: assistant-3kknR8914kSenLpWZ6tUcL\n",
      "  Uploaded 32/50: assistant-8KmS3LLTHwHwLfvTiGw8Ab\n",
      "  Uploaded 33/50: assistant-VQQ6pQ5GUSSNqL4qoaXH7y\n",
      "  Uploaded 34/50: assistant-Hdzq8njkrvA7BUxF9bihK1\n",
      "  Uploaded 35/50: assistant-2WeUa1zRfyJex2g1C9nikM\n",
      "  Uploaded 36/50: assistant-GQKsJ6B5dWYDJjQjU9AmLH\n",
      "  Uploaded 37/50: assistant-H7ZszgQw3XxRE11pd987XG\n",
      "  Uploaded 38/50: assistant-YZ2GgMHHcqyzzadCtk2Vfn\n",
      "  Uploaded 39/50: assistant-NNUCt1e2KaAxzgG2RmoYor\n",
      "  Uploaded 40/50: assistant-EvRGFPjEq3W79ZDcbGkLiZ\n",
      "  Uploaded 41/50: assistant-1SfpudDsy9fXiN3MjxrWfc\n",
      "  Uploaded 42/50: assistant-4KfskcDmenCe6RgJNxfa4Q\n",
      "  Uploaded 43/50: assistant-NbJEqwTh6eem4jzCTxV4K8\n",
      "  Uploaded 44/50: assistant-JGMni8TVo8qvpWD2EPF5sK\n",
      "  Uploaded 45/50: assistant-6squP5RmKHtmi4NSP98eqa\n",
      "  Uploaded 46/50: assistant-VRP3ojoySWBFPj1Luh6qxN\n",
      "  Uploaded 47/50: assistant-KYfCPUmipBBpuKy5ybLFnC\n",
      "  Uploaded 48/50: assistant-P2xPvYR5W6QUhrVBjY15t7\n",
      "  Uploaded 49/50: assistant-UqAoneoRJiWif81i6orXrv\n",
      "  Uploaded 50/50: assistant-TepM592qwfCYztTKHfHCus\n",
      "Individual upload completed: 50 files in 83.48s\n",
      "Rate: 0.60 files/second\n"
     ]
    }
   ],
   "source": [
    "# Upload files to Azure OpenAI individually\n",
    "def upload_files_individually(file_paths):\n",
    "    print(f\"Uploading {len(file_paths)} files individually to OpenAI...\")\n",
    "    file_ids = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                file_obj = client.files.create(file=f, purpose=\"assistants\")\n",
    "                file_ids.append(file_obj.id)\n",
    "                print(f\"  Uploaded {i+1}/{len(file_paths)}: {file_obj.id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to upload file {i+1}: {e}\")\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Individual upload completed: {len(file_ids)} files in {duration:.2f}s\")\n",
    "    print(f\"Rate: {len(file_ids)/duration:.2f} files/second\")\n",
    "    return file_ids, duration\n",
    "\n",
    "individual_file_ids, individual_duration = upload_files_individually(test_files[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27049225",
   "metadata": {},
   "source": [
    "### Option 2: Individual Upload of Files to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ce7e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 50 files to vector store individually...\n",
      "  Added 1/50: assistant-31psNk2GF9gGb5CxLLBbGm\n",
      "  Added 2/50: assistant-CgtUGQ4uQeEdrJmbxDYBcx\n",
      "  Added 3/50: assistant-DhddtZi9icff3GcDp6BHtk\n",
      "  Added 4/50: assistant-NAZb2Dq1Qk1pViA1GKop6g\n",
      "  Added 5/50: assistant-Ryg7X5uNaztaNaB7eMEFju\n",
      "  Added 6/50: assistant-6mQfZccYc2mBgUKcGX5umu\n",
      "  Added 7/50: assistant-WfgdaX8Sdghrqqm2qtAjVW\n",
      "  Added 8/50: assistant-UEsw4UFjz1QTEfGeEtzqXt\n",
      "  Added 9/50: assistant-HWmwVcrqzQdoZejvJqt8fH\n",
      "  Added 10/50: assistant-P6JE5oKdhZJEmLh983JPFH\n",
      "  Added 11/50: assistant-83MtnFYXYnfrJCRJqWmwxH\n",
      "  Added 12/50: assistant-Md4eBZveZnA7FG27odCtkY\n",
      "  Added 13/50: assistant-7Fww5FtDayAPJZRUtC1p9c\n",
      "  Added 14/50: assistant-6mwWuH5TAnL2zWxd1f9Ckg\n",
      "  Added 15/50: assistant-EZuzoxBihWRmdZqAKQon7X\n",
      "  Added 16/50: assistant-Pxyjw8QdfeEMAFc4KaRZS9\n",
      "  Added 17/50: assistant-J7xr4JdNKhaXB7nx1wMN4q\n",
      "  Added 18/50: assistant-9czfE5aB43wp3apME8B3sP\n",
      "  Added 19/50: assistant-QMBseBoUQuBUPq7zW9nMk5\n",
      "  Added 20/50: assistant-Kxo4rT75HHHcSSJAFymdkK\n",
      "  Added 21/50: assistant-SjwGHnhHVHicd5SPVdeBqh\n",
      "  Added 22/50: assistant-SVzCSJ2X5X64V85wdrqFqB\n",
      "  Added 23/50: assistant-7TnBdykAogFqjmiboqjygz\n",
      "  Added 24/50: assistant-43orgQBfeLbX5qGvY4E42R\n",
      "  Added 25/50: assistant-DGpog2fEiY2abUW9zWn1j1\n",
      "  Added 26/50: assistant-PWi2cY9LKcTayCW2VsSyEq\n",
      "  Added 27/50: assistant-Jd7jjT9DnMmQNj98G5pDTM\n",
      "  Added 28/50: assistant-Ka2viKs8knFbKpsZLfxkJM\n",
      "  Added 29/50: assistant-EEPw3L7GKV8iSJPEzDSRVU\n",
      "  Added 30/50: assistant-7HnDFKrov576iMZNzeP5ca\n",
      "  Added 31/50: assistant-3kknR8914kSenLpWZ6tUcL\n",
      "  Added 32/50: assistant-8KmS3LLTHwHwLfvTiGw8Ab\n",
      "  Added 33/50: assistant-VQQ6pQ5GUSSNqL4qoaXH7y\n",
      "  Added 34/50: assistant-Hdzq8njkrvA7BUxF9bihK1\n",
      "  Added 35/50: assistant-2WeUa1zRfyJex2g1C9nikM\n",
      "  Added 36/50: assistant-GQKsJ6B5dWYDJjQjU9AmLH\n",
      "  Added 37/50: assistant-H7ZszgQw3XxRE11pd987XG\n",
      "  Added 38/50: assistant-YZ2GgMHHcqyzzadCtk2Vfn\n",
      "  Added 39/50: assistant-NNUCt1e2KaAxzgG2RmoYor\n",
      "  Added 40/50: assistant-EvRGFPjEq3W79ZDcbGkLiZ\n",
      "  Added 41/50: assistant-1SfpudDsy9fXiN3MjxrWfc\n",
      "  Added 42/50: assistant-4KfskcDmenCe6RgJNxfa4Q\n",
      "  Added 43/50: assistant-NbJEqwTh6eem4jzCTxV4K8\n",
      "  Added 44/50: assistant-JGMni8TVo8qvpWD2EPF5sK\n",
      "  Added 45/50: assistant-6squP5RmKHtmi4NSP98eqa\n",
      "  Added 46/50: assistant-VRP3ojoySWBFPj1Luh6qxN\n",
      "  Added 47/50: assistant-KYfCPUmipBBpuKy5ybLFnC\n",
      "  Added 48/50: assistant-P2xPvYR5W6QUhrVBjY15t7\n",
      "  Added 49/50: assistant-UqAoneoRJiWif81i6orXrv\n",
      "  Added 50/50: assistant-TepM592qwfCYztTKHfHCus\n",
      "Individual vector store addition: 50 files in 23.68s\n",
      "Rate: 2.11 files/second\n"
     ]
    }
   ],
   "source": [
    "# Add files to Vector Store individually\n",
    "def add_files_to_vector_store_individually(file_ids, vector_store_id):\n",
    "    print(f\"Adding {len(file_ids)} files to vector store individually...\")\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "    \n",
    "    for i, file_id in enumerate(file_ids):\n",
    "        try:\n",
    "            result = client.vector_stores.files.create(\n",
    "                vector_store_id=vector_store_id,\n",
    "                file_id=file_id\n",
    "            )\n",
    "            successful += 1\n",
    "            print(f\"  Added {i+1}/{len(file_ids)}: {result.id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to add file {i+1}: {e}\")\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Individual vector store addition: {successful} files in {duration:.2f}s\")\n",
    "    print(f\"Rate: {successful/duration:.2f} files/second\")\n",
    "    return successful, duration\n",
    "\n",
    "individual_vs_count, individual_vs_duration = add_files_to_vector_store_individually(individual_file_ids, vector_store_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe03c4d",
   "metadata": {},
   "source": [
    "### Option 3: Batch Upload of Files to Azure OpenAI and its Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cb4f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch uploading 50 files in batches of 20...\n",
      "Uploading batch 1: files 1-20\n",
      "  Uploaded 20 files in 31.07s\n",
      "Uploading batch 2: files 21-40\n",
      "  Uploaded 20 files in 27.44s\n",
      "Uploading batch 3: files 41-50\n",
      "  Uploaded 10 files in 14.94s\n",
      "Creating vector store batch with 50 files...\n",
      "Vector store batch created: vsfb_3626cf46a360475392876cd251c7d13e\n",
      "Batch creation time: 2.10s\n",
      "Total time: 75.55s\n"
     ]
    }
   ],
   "source": [
    "# Upload files to OpenAI and Vector Store\n",
    "def batch_upload_to_vector_store(file_paths, batch_size=20):\n",
    "    print(f\"Batch uploading {len(file_paths)} files in batches of {batch_size}...\")\n",
    "    all_file_ids = []\n",
    "    total_upload_time = 0\n",
    "    \n",
    "    # Step 1: Upload all files to OpenAI first\n",
    "    for i in range(0, len(file_paths), batch_size):\n",
    "        batch_files = file_paths[i:i+batch_size]\n",
    "        print(f\"Uploading batch {i//batch_size + 1}: files {i+1}-{i+len(batch_files)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        batch_file_ids = []\n",
    "        \n",
    "        for file_path in batch_files:\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    file_obj = client.files.create(file=f, purpose=\"assistants\")\n",
    "                    batch_file_ids.append(file_obj.id)\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed to upload file: {e}\")\n",
    "        \n",
    "        upload_time = time.time() - start_time\n",
    "        total_upload_time += upload_time\n",
    "        all_file_ids.extend(batch_file_ids)\n",
    "        \n",
    "        print(f\"  Uploaded {len(batch_file_ids)} files in {upload_time:.2f}s\")\n",
    "    \n",
    "    # Step 2: Create vector store batch\n",
    "    print(f\"Creating vector store batch with {len(all_file_ids)} files...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        file_batch = client.vector_stores.file_batches.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_ids=all_file_ids\n",
    "        )\n",
    "        batch_creation_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Vector store batch created: {file_batch.id}\")\n",
    "        print(f\"Batch creation time: {batch_creation_time:.2f}s\")\n",
    "        print(f\"Total time: {total_upload_time + batch_creation_time:.2f}s\")\n",
    "        \n",
    "        return file_batch, total_upload_time, batch_creation_time, len(all_file_ids)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create vector store batch: {e}\")\n",
    "        return None, total_upload_time, 0, len(all_file_ids)\n",
    "\n",
    "# Test 2: Batch upload next 50 files\n",
    "batch_result, batch_upload_time, batch_creation_time, batch_file_count = batch_upload_to_vector_store(test_files[50:100], batch_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d72cf",
   "metadata": {},
   "source": [
    "### Option 4: Concurrent Upload Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea7bcd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent upload test: 50 files with 30 workers\n",
      "  File 1: SUCCESS (0.90s)\n",
      "  File 2: SUCCESS (2.39s)\n",
      "  File 3: SUCCESS (1.56s)\n",
      "  File 4: SUCCESS (2.15s)\n",
      "  File 5: SUCCESS (3.26s)\n",
      "  File 6: SUCCESS (3.21s)\n",
      "  File 7: SUCCESS (2.55s)\n",
      "  File 8: SUCCESS (1.78s)\n",
      "  File 9: SUCCESS (1.74s)\n",
      "  File 10: SUCCESS (2.64s)\n",
      "  File 11: SUCCESS (2.12s)\n",
      "  File 12: SUCCESS (1.59s)\n",
      "  File 13: SUCCESS (1.74s)\n",
      "  File 14: SUCCESS (2.05s)\n",
      "  File 15: SUCCESS (1.90s)\n",
      "  File 16: SUCCESS (2.21s)\n",
      "  File 17: SUCCESS (2.18s)\n",
      "  File 18: SUCCESS (2.04s)\n",
      "  File 19: SUCCESS (1.95s)\n",
      "  File 20: SUCCESS (1.23s)\n",
      "  File 21: SUCCESS (2.54s)\n",
      "  File 22: SUCCESS (1.34s)\n",
      "  File 23: SUCCESS (2.99s)\n",
      "  File 24: SUCCESS (1.64s)\n",
      "  File 25: SUCCESS (3.11s)\n",
      "  File 26: SUCCESS (3.72s)\n",
      "  File 27: SUCCESS (3.55s)\n",
      "  File 28: SUCCESS (3.23s)\n",
      "  File 29: SUCCESS (1.61s)\n",
      "  File 30: SUCCESS (1.58s)\n",
      "  File 31: SUCCESS (2.97s)\n",
      "  File 32: SUCCESS (4.06s)\n",
      "  File 33: SUCCESS (1.36s)\n",
      "  File 34: SUCCESS (1.62s)\n",
      "  File 35: SUCCESS (1.58s)\n",
      "  File 36: SUCCESS (0.89s)\n",
      "  File 37: SUCCESS (2.57s)\n",
      "  File 38: SUCCESS (1.42s)\n",
      "  File 39: SUCCESS (1.88s)\n",
      "  File 40: SUCCESS (2.19s)\n",
      "  File 41: SUCCESS (0.99s)\n",
      "  File 42: SUCCESS (2.28s)\n",
      "  File 43: SUCCESS (2.17s)\n",
      "  File 44: SUCCESS (2.16s)\n",
      "  File 45: SUCCESS (1.94s)\n",
      "  File 46: SUCCESS (0.93s)\n",
      "  File 47: SUCCESS (2.80s)\n",
      "  File 48: SUCCESS (2.70s)\n",
      "  File 49: SUCCESS (1.85s)\n",
      "  File 50: SUCCESS (1.85s)\n",
      "Concurrent upload results:\n",
      "  Total files: 50\n",
      "  Successful: 50\n",
      "  Failed: 0\n",
      "  Total time: 5.32s\n",
      "  Rate: 9.40 files/second\n",
      "  Rate limit errors (429): 0\n"
     ]
    }
   ],
   "source": [
    "# Upload files concurrently\n",
    "def concurrent_upload_test(file_paths, max_workers=30):\n",
    "    print(f\"Concurrent upload test: {len(file_paths)} files with {max_workers} workers\")\n",
    "    \n",
    "    def upload_single_file(file_path):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                file_obj = client.files.create(file=f, purpose=\"assistants\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'file_id': file_obj.id,\n",
    "                'duration': time.time() - start_time,\n",
    "                'error': None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'file_id': None,\n",
    "                'duration': time.time() - start_time,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(upload_single_file, fp) for fp in file_paths]\n",
    "        \n",
    "        for i, future in enumerate(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            status = \"SUCCESS\" if result['success'] else f\"FAILED: {result['error'][:30]}\"\n",
    "            print(f\"  File {i+1}: {status} ({result['duration']:.2f}s)\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    print(f\"Concurrent upload results:\")\n",
    "    print(f\"  Total files: {len(file_paths)}\")\n",
    "    print(f\"  Successful: {len(successful)}\")\n",
    "    print(f\"  Failed: {len(failed)}\")\n",
    "    print(f\"  Total time: {total_time:.2f}s\")\n",
    "    print(f\"  Rate: {len(successful)/total_time:.2f} files/second\")\n",
    "    \n",
    "    # Count rate limit errors\n",
    "    rate_limit_errors = sum(1 for r in failed if '429' in str(r['error']))\n",
    "    print(f\"  Rate limit errors (429): {rate_limit_errors}\")\n",
    "    \n",
    "    return [r['file_id'] for r in successful], len(successful), total_time\n",
    "\n",
    "concurrent_file_ids, concurrent_success_count, concurrent_duration = concurrent_upload_test(test_files[100:150], max_workers=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896a5f5",
   "metadata": {},
   "source": [
    "### High Concurrency Stress Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9029a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High concurrency stress test (50 files, 50 workers)\n",
      "Concurrent upload test: 50 files with 50 workers\n",
      "  File 1: SUCCESS (5.59s)\n",
      "  File 2: SUCCESS (2.61s)\n",
      "  File 3: SUCCESS (1.49s)\n",
      "  File 4: SUCCESS (5.08s)\n",
      "  File 5: SUCCESS (3.04s)\n",
      "  File 6: SUCCESS (1.50s)\n",
      "  File 7: SUCCESS (1.97s)\n",
      "  File 8: SUCCESS (2.66s)\n",
      "  File 9: SUCCESS (3.93s)\n",
      "  File 10: SUCCESS (2.76s)\n",
      "  File 11: SUCCESS (4.96s)\n",
      "  File 12: SUCCESS (2.76s)\n",
      "  File 13: SUCCESS (3.93s)\n",
      "  File 14: SUCCESS (4.89s)\n",
      "  File 15: SUCCESS (3.62s)\n",
      "  File 16: SUCCESS (1.90s)\n",
      "  File 17: SUCCESS (3.67s)\n",
      "  File 18: SUCCESS (3.18s)\n",
      "  File 19: SUCCESS (2.38s)\n",
      "  File 20: SUCCESS (4.15s)\n",
      "  File 21: SUCCESS (1.52s)\n",
      "  File 22: SUCCESS (2.44s)\n",
      "  File 23: SUCCESS (2.03s)\n",
      "  File 24: SUCCESS (3.80s)\n",
      "  File 25: SUCCESS (3.44s)\n",
      "  File 26: SUCCESS (1.60s)\n",
      "  File 27: SUCCESS (1.78s)\n",
      "  File 28: SUCCESS (2.02s)\n",
      "  File 29: SUCCESS (3.86s)\n",
      "  File 30: SUCCESS (2.36s)\n",
      "  File 31: SUCCESS (3.34s)\n",
      "  File 32: SUCCESS (1.66s)\n",
      "  File 33: SUCCESS (2.87s)\n",
      "  File 34: SUCCESS (4.20s)\n",
      "  File 35: SUCCESS (4.27s)\n",
      "  File 36: SUCCESS (4.76s)\n",
      "  File 37: SUCCESS (1.48s)\n",
      "  File 38: SUCCESS (2.80s)\n",
      "  File 39: SUCCESS (2.76s)\n",
      "  File 40: SUCCESS (1.50s)\n",
      "  File 41: SUCCESS (1.10s)\n",
      "  File 42: SUCCESS (1.71s)\n",
      "  File 43: SUCCESS (2.28s)\n",
      "  File 44: SUCCESS (2.69s)\n",
      "  File 45: SUCCESS (1.65s)\n",
      "  File 46: SUCCESS (2.36s)\n",
      "  File 47: SUCCESS (2.96s)\n",
      "  File 48: SUCCESS (2.93s)\n",
      "  File 49: SUCCESS (3.65s)\n",
      "  File 50: SUCCESS (1.82s)\n",
      "Concurrent upload results:\n",
      "  Total files: 50\n",
      "  Successful: 50\n",
      "  Failed: 0\n",
      "  Total time: 5.60s\n",
      "  Rate: 8.92 files/second\n",
      "  Rate limit errors (429): 0\n"
     ]
    }
   ],
   "source": [
    "# Perform high concurrency stress test\n",
    "print(\"High concurrency stress test (50 files, 50 workers)\")\n",
    "stress_file_ids, stress_success_count, stress_duration = concurrent_upload_test(test_files[150:200], max_workers=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ab97c",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76cbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status of batch: vsfb_3626cf46a360475392876cd251c7d13e\n",
      "Batch status: completed\n",
      "File counts - Total: 50, Completed: 50\n",
      "In progress: 0, Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Check vector store file batch status\n",
    "def check_batch_status(batch_id):\n",
    "    if not batch_id:\n",
    "        print(\"No batch ID to check\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Checking status of batch: {batch_id}\")\n",
    "    try:\n",
    "        batch = client.vector_stores.file_batches.retrieve(\n",
    "            vector_store_id=vector_store_id,\n",
    "            batch_id=batch_id\n",
    "        )\n",
    "        \n",
    "        print(f\"Batch status: {batch.status}\")\n",
    "        print(f\"File counts - Total: {batch.file_counts.total}, Completed: {batch.file_counts.completed}\")\n",
    "        print(f\"In progress: {batch.file_counts.in_progress}, Failed: {batch.file_counts.failed}\")\n",
    "        \n",
    "        return batch.status\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to check batch status: {e}\")\n",
    "        return None\n",
    "\n",
    "if batch_result:\n",
    "    batch_status = check_batch_status(batch_result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87b4e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in vector store...\n",
      "Total files in vector store: 20\n",
      "File status counts:\n",
      "  completed: 20\n"
     ]
    }
   ],
   "source": [
    "# List vector store files\n",
    "def list_vector_store_files():\n",
    "    print(\"Listing files in vector store...\")\n",
    "    try:\n",
    "        files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "        print(f\"Total files in vector store: {len(files.data)}\")\n",
    "        \n",
    "        status_counts = {}\n",
    "        for file in files.data:\n",
    "            status = file.status\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        \n",
    "        print(\"File status counts:\")\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to list vector store files: {e}\")\n",
    "\n",
    "list_vector_store_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2aa38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "Test 1 - Individual Upload + Individual Vector Store Addition (50 files):\n",
      "  File upload: 0.60 files/sec\n",
      "  Vector store addition: 2.11 files/sec\n",
      "  Total time: 107.16s\n",
      "\n",
      "Test 2 - Batch Upload + Vector Store Batch (50 files):\n",
      "  File upload: 0.68 files/sec\n",
      "  Vector store batch creation: 2.10s\n",
      "  Total time: 75.55s\n",
      "\n",
      "Test 3 - Concurrent Upload (50 files, 30 workers):\n",
      "  Success rate: 50/50 files\n",
      "  Upload rate: 9.40 files/sec\n",
      "\n",
      "Test 4 - High Concurrency Stress Test (50 files, 50 workers):\n",
      "  Success rate: 50/50 files\n",
      "  Upload rate: 8.92 files/sec\n",
      "\n",
      "Overall Results:\n",
      "  Total files processed: 200/200\n",
      "  Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Performance Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Test 1 - Individual Upload + Individual Vector Store Addition (50 files):\")\n",
    "print(f\"  File upload: {50/individual_duration:.2f} files/sec\")\n",
    "print(f\"  Vector store addition: {individual_vs_count/individual_vs_duration:.2f} files/sec\")\n",
    "print(f\"  Total time: {individual_duration + individual_vs_duration:.2f}s\")\n",
    "\n",
    "print(f\"\\nTest 2 - Batch Upload + Vector Store Batch (50 files):\")\n",
    "print(f\"  File upload: {batch_file_count/batch_upload_time:.2f} files/sec\")\n",
    "print(f\"  Vector store batch creation: {batch_creation_time:.2f}s\")\n",
    "print(f\"  Total time: {batch_upload_time + batch_creation_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nTest 3 - Concurrent Upload (50 files, 30 workers):\")\n",
    "print(f\"  Success rate: {concurrent_success_count}/50 files\")\n",
    "print(f\"  Upload rate: {concurrent_success_count/concurrent_duration:.2f} files/sec\")\n",
    "\n",
    "print(f\"\\nTest 4 - High Concurrency Stress Test (50 files, 50 workers):\")\n",
    "print(f\"  Success rate: {stress_success_count}/50 files\")\n",
    "print(f\"  Upload rate: {stress_success_count/stress_duration:.2f} files/sec\")\n",
    "\n",
    "# Calculate theoretical vs actual rates\n",
    "total_successful_files = individual_vs_count + batch_file_count + concurrent_success_count + stress_success_count\n",
    "print(f\"\\nOverall Results:\")\n",
    "print(f\"  Total files processed: {total_successful_files}/200\")\n",
    "print(f\"  Success rate: {total_successful_files/200*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c874d3",
   "metadata": {},
   "source": [
    "### Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f73c3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "Deleted test files directory: c:\\Users\\lturakulov\\Downloads\\ZZZ_TEMP\\ZZZ_CASES_WITH_RCA\\Bosch_File_Uploads\\test_files\n",
      "Deleting uploaded files from Azure OpenAI...\n",
      "Found 170 unique files to delete\n",
      "  Deleted file: assistant-DQxdfivWJyhL1hDMUKRyi5\n",
      "  Deleted file: assistant-UJfr7qM53829R28L25vVyE\n",
      "  Deleted file: assistant-A84a1vAmEfpDQ5hFxTN1C6\n",
      "  Deleted file: assistant-6mwWuH5TAnL2zWxd1f9Ckg\n",
      "  Deleted file: assistant-Ro9eZvwU7BxeQcx4MQEcqD\n",
      "  Deleted file: assistant-FoqX8wSVqvZChFFwyeEtFa\n",
      "  Deleted file: assistant-Jd7jjT9DnMmQNj98G5pDTM\n",
      "  Deleted file: assistant-Bi2Ridf38WkupBne1gbDe1\n",
      "  Deleted file: assistant-6JVzUDuYeSXALb3NazMAdf\n",
      "  Deleted file: assistant-4WS7BvcYtUUJY48ZUHUqF1\n",
      "  Deleted file: assistant-1jDEQuseGqnYvadUFqLwM2\n",
      "  Deleted file: assistant-1cEG83zsUmDuS4H7xnnAGm\n",
      "  Deleted file: assistant-Kxo4rT75HHHcSSJAFymdkK\n",
      "  Deleted file: assistant-T1jcYbEt5dCtrWDwkUacF5\n",
      "  Deleted file: assistant-4zhMKZumJY9HANVt4xWpiN\n",
      "  Deleted file: assistant-GSwctpRL9jscKk6GW3XroM\n",
      "  Deleted file: assistant-EfmbAAhL4jCKuAyVJSbzfR\n",
      "  Deleted file: assistant-VV47BdRg2jUSQBvsWRkEMC\n",
      "  Deleted file: assistant-FqUSjrdg84YyszGM6bnyox\n",
      "  Deleted file: assistant-LurkCmo6gj3i55UYJsSAgV\n",
      "  Deleted file: assistant-Vnk3yvrFUpngUfJhh72Y8k\n",
      "  Deleted file: assistant-Ka2viKs8knFbKpsZLfxkJM\n",
      "  Deleted file: assistant-VGSxDQsM6aMgXGZMtTnyrd\n",
      "  Deleted file: assistant-HWmwVcrqzQdoZejvJqt8fH\n",
      "  Deleted file: assistant-43orgQBfeLbX5qGvY4E42R\n",
      "  Deleted file: assistant-WfgdaX8Sdghrqqm2qtAjVW\n",
      "  Deleted file: assistant-HTrgkNndqxEdXTk7W28PbT\n",
      "  Deleted file: assistant-JGMni8TVo8qvpWD2EPF5sK\n",
      "  Deleted file: assistant-VRP3ojoySWBFPj1Luh6qxN\n",
      "  Deleted file: assistant-MJWcNTz5GfUDaWanyB8o3Q\n",
      "  Deleted file: assistant-SdkRgt9ALLAgUyFKs32rTt\n",
      "  Deleted file: assistant-4KfskcDmenCe6RgJNxfa4Q\n",
      "  Deleted file: assistant-GQKsJ6B5dWYDJjQjU9AmLH\n",
      "  Deleted file: assistant-H7ZszgQw3XxRE11pd987XG\n",
      "  Deleted file: assistant-P2xPvYR5W6QUhrVBjY15t7\n",
      "  Deleted file: assistant-FGTURCtxLKCrZSYrrNHuTv\n",
      "  Deleted file: assistant-DGpog2fEiY2abUW9zWn1j1\n",
      "  Deleted file: assistant-DhddtZi9icff3GcDp6BHtk\n",
      "  Deleted file: assistant-J1KQoMhvSL2GetGr2kDEJv\n",
      "  Deleted file: assistant-PCvE5By7SoddHDkMLnv4ss\n",
      "  Deleted file: assistant-7TnBdykAogFqjmiboqjygz\n",
      "  Deleted file: assistant-VUMzthRfLs3UckMbtZfgNm\n",
      "  Deleted file: assistant-VgSWvByx6e861U5yg3Su8a\n",
      "  Deleted file: assistant-2WeUa1zRfyJex2g1C9nikM\n",
      "  Deleted file: assistant-ANizRfBxgab8mdrCuuJw9g\n",
      "  Deleted file: assistant-2puVm5EXeZxXP2YCTBE862\n",
      "  Deleted file: assistant-3PrKxE2dJaNjYZTn53YYJY\n",
      "  Deleted file: assistant-2pTriUA98KaQ6gyQ6MGz5A\n",
      "  Deleted file: assistant-6uj9a34FGm3f1taLQpdAin\n",
      "  Deleted file: assistant-XwTKcNHz92vD2FhCAzfWSF\n",
      "  Deleted file: assistant-MXqEEuUGCioMdtEudrBqng\n",
      "  Deleted file: assistant-N65k4gGDzaKvgHABAFmfvF\n",
      "  Deleted file: assistant-2ezFhiAP6QkLkXDQ2XoB7q\n",
      "  Deleted file: assistant-GNZdyGxjL3k91zapyLb67Y\n",
      "  Deleted file: assistant-XH7JzmT6NdGRdAG5FbuqiL\n",
      "  Deleted file: assistant-JsntmQhpYNEmSNLf4Vm3bG\n",
      "  Deleted file: assistant-CZaEnGerjsz4kizZYH1pCe\n",
      "  Deleted file: assistant-CgtUGQ4uQeEdrJmbxDYBcx\n",
      "  Deleted file: assistant-RugMPvqufNcvTSnaycm54t\n",
      "  Deleted file: assistant-SwJuyc6RnRtxiM69AEUqLZ\n",
      "  Deleted file: assistant-Ft1XjUpD8sYzbPq1cwteKM\n",
      "  Deleted file: assistant-6qiK1idZW6d85v5VgoXqiY\n",
      "  Deleted file: assistant-YZ2GgMHHcqyzzadCtk2Vfn\n",
      "  Deleted file: assistant-9MgrirCUEPcqpGnnf1DKUE\n",
      "  Deleted file: assistant-PWi2cY9LKcTayCW2VsSyEq\n",
      "  Deleted file: assistant-8KmS3LLTHwHwLfvTiGw8Ab\n",
      "  Deleted file: assistant-5smpox9wcc6JnFH6KyLUqg\n",
      "  Deleted file: assistant-7Fww5FtDayAPJZRUtC1p9c\n",
      "  Deleted file: assistant-6squP5RmKHtmi4NSP98eqa\n",
      "  Deleted file: assistant-NbJEqwTh6eem4jzCTxV4K8\n",
      "  Deleted file: assistant-QMTNKJfxce8uQSDpNVG6vd\n",
      "  Deleted file: assistant-QMBseBoUQuBUPq7zW9nMk5\n",
      "  Deleted file: assistant-7HnDFKrov576iMZNzeP5ca\n",
      "  Deleted file: assistant-NNUCt1e2KaAxzgG2RmoYor\n",
      "  Deleted file: assistant-EvRGFPjEq3W79ZDcbGkLiZ\n",
      "  Deleted file: assistant-KYfCPUmipBBpuKy5ybLFnC\n",
      "  Deleted file: assistant-Ryg7X5uNaztaNaB7eMEFju\n",
      "  Deleted file: assistant-EcSoZnm5YbgmjnfndRop9n\n",
      "  Deleted file: assistant-STAUadjBkRaSTNE3rkStmZ\n",
      "  Deleted file: assistant-NDroyGFNQZS5TaizaRCR4F\n",
      "  Deleted file: assistant-467x58oiZJHKTm7UFv6EX3\n",
      "  Deleted file: assistant-7PaQZ8MpAmcRzxMCnhUfsG\n",
      "  Deleted file: assistant-HFNwuP7pJ6YUtR11hTfKec\n",
      "  Deleted file: assistant-LGbkD7ZWe9b8RAB6T5dFhH\n",
      "  Deleted file: assistant-CXFEH2fopH2YGZ3WpsX5PZ\n",
      "  Deleted file: assistant-EJostiLh6bXWzYpPi3Ki9K\n",
      "  Deleted file: assistant-VQQ6pQ5GUSSNqL4qoaXH7y\n",
      "  Deleted file: assistant-NAZb2Dq1Qk1pViA1GKop6g\n",
      "  Deleted file: assistant-Jdys6QRhcoH8NPZXf9Ai3F\n",
      "  Deleted file: assistant-76a4Xu656cnd4LizoB9SvX\n",
      "  Deleted file: assistant-5bfLyHn3TPr6wWBa3u1z8H\n",
      "  Deleted file: assistant-9tjtL8KMZeMAYA27BPh79r\n",
      "  Deleted file: assistant-9AePhJKv1Y9JGbPn3HPqBY\n",
      "  Deleted file: assistant-7o5Tw1YsVc2nbcyu8hFh2A\n",
      "  Deleted file: assistant-LbHJzptK4PmSW9iQ3hYfkP\n",
      "  Deleted file: assistant-T1S2YNuJ9p6rv27ttta1qq\n",
      "  Deleted file: assistant-PsrAC9SwrsDiKTwiKHrAVz\n",
      "  Deleted file: assistant-FJ16mxmdaYYZxG1P5M6E6v\n",
      "  Deleted file: assistant-DhDHrJJsEou17sZKG1fq1G\n",
      "  Deleted file: assistant-By57Qn2QBGCF6EopaM6han\n",
      "  Deleted file: assistant-MYSG2H2CrViLDTHRV7MwSo\n",
      "  Deleted file: assistant-EZuzoxBihWRmdZqAKQon7X\n",
      "  Deleted file: assistant-P6JE5oKdhZJEmLh983JPFH\n",
      "  Deleted file: assistant-TepM592qwfCYztTKHfHCus\n",
      "  Deleted file: assistant-Cv16BWmeaAAeTgNWaWGrYG\n",
      "  Deleted file: assistant-9czfE5aB43wp3apME8B3sP\n",
      "  Deleted file: assistant-E4DbrbUSGsTnRbW5zSbtEU\n",
      "  Deleted file: assistant-Y1PmcmCoWL2ZawefkJVxLJ\n",
      "  Deleted file: assistant-3kknR8914kSenLpWZ6tUcL\n",
      "  Deleted file: assistant-XsX2YpNkA4xddRJfjS4QEA\n",
      "  Deleted file: assistant-XAaFmeSDv1yGCSHzMb3UjY\n",
      "  Deleted file: assistant-CDKtuoLLmcugX1JfLH96fg\n",
      "  Deleted file: assistant-HW6x3CikoD5E7XbWxhbMPN\n",
      "  Deleted file: assistant-SjwGHnhHVHicd5SPVdeBqh\n",
      "  Deleted file: assistant-PvwC4d3ZBjs6QL311aLDYQ\n",
      "  Deleted file: assistant-UY4J9fNvUW7Z4HwQnPKXNX\n",
      "  Deleted file: assistant-XXvPsFg9LTkQ6jBvM3fkde\n",
      "  Deleted file: assistant-EEPw3L7GKV8iSJPEzDSRVU\n",
      "  Deleted file: assistant-31psNk2GF9gGb5CxLLBbGm\n",
      "  Deleted file: assistant-J19VzdfxAaUbdXXXntx6jo\n",
      "  Deleted file: assistant-JF44LZVDx3evKWrfjSeFnh\n",
      "  Deleted file: assistant-J7xr4JdNKhaXB7nx1wMN4q\n",
      "  Deleted file: assistant-1CKbUvbtDyVZsGLdnZwjLS\n",
      "  Deleted file: assistant-TARPVP2dofGqV7ZPByt7He\n",
      "  Deleted file: assistant-5VydFEizd3cBdWWhzfFGy6\n",
      "  Deleted file: assistant-REne63zrPzQguD8QVcRGeM\n",
      "  Deleted file: assistant-G7m7fKEDXtP1f42LHeYvxB\n",
      "  Deleted file: assistant-9fbjGzU3uZgxWMmg7geU3G\n",
      "  Deleted file: assistant-4eiuPwaT7FdupQQfXiPgsr\n",
      "  Deleted file: assistant-UqAoneoRJiWif81i6orXrv\n",
      "  Deleted file: assistant-3y53GVTdCz3D2qC8GVGaNy\n",
      "  Deleted file: assistant-TCNDnbyLY7upZSZnKqdFRF\n",
      "  Deleted file: assistant-2rp5SknbAwSt6SFG8N1PRW\n",
      "  Deleted file: assistant-ErGJuf8wv8naF6MnC83isa\n",
      "  Deleted file: assistant-4hnAE2TrTvCJ2jsSxysxUP\n",
      "  Deleted file: assistant-NtwYCJ4wiYrJRoQA7HPrHE\n",
      "  Deleted file: assistant-EkXcZYLaBCzn3sEpwYNUfi\n",
      "  Deleted file: assistant-WWSzmBLuqAxAkptw4HH4RL\n",
      "  Deleted file: assistant-1uXhyJedFZxGjhJp82cnff\n",
      "  Deleted file: assistant-SVzCSJ2X5X64V85wdrqFqB\n",
      "  Deleted file: assistant-6mQfZccYc2mBgUKcGX5umu\n",
      "  Deleted file: assistant-Pxyjw8QdfeEMAFc4KaRZS9\n",
      "  Deleted file: assistant-KXQqPutV8hEsyiDsMzpDd3\n",
      "  Deleted file: assistant-5DRYqnSQJyuMQV38XRzcQm\n",
      "  Deleted file: assistant-83MtnFYXYnfrJCRJqWmwxH\n",
      "  Deleted file: assistant-JyYCpdkNPhfAXscRrbgLvS\n",
      "  Deleted file: assistant-Qod4xh453AavRqSeeNXi2p\n",
      "  Deleted file: assistant-2wtx64DFf7pGonqToCF4PK\n",
      "  Deleted file: assistant-Hxhw7rc6mKPSKtpZqzeg2u\n",
      "  Deleted file: assistant-UEsw4UFjz1QTEfGeEtzqXt\n",
      "  Deleted file: assistant-6P2piy9LHFpAjnoS4zPJeR\n",
      "  Deleted file: assistant-X9dwGXUZnKKjMtRQ8bGJ2L\n",
      "  Deleted file: assistant-UZgfHtxMwm2YVq5DVMsdug\n",
      "  Deleted file: assistant-Md4eBZveZnA7FG27odCtkY\n",
      "  Deleted file: assistant-SNemoneXstoADq6meLEfsk\n",
      "  Deleted file: assistant-BiqsbnLQq4rcVftScWxETP\n",
      "  Deleted file: assistant-WqB93CjoPaojj9kWFRwYEP\n",
      "  Deleted file: assistant-FQKXFrU2e81uByCrL32Dj9\n",
      "  Deleted file: assistant-1SfpudDsy9fXiN3MjxrWfc\n",
      "  Deleted file: assistant-6RrCJY1mpfh3gVeiFCCskd\n",
      "  Deleted file: assistant-78rw6stBHMKaVQo5BhKAar\n",
      "  Deleted file: assistant-LwjDH2bqXE1vhPMmbwFz4D\n",
      "  Deleted file: assistant-K2EJU3SPV2795nGCRWQrKG\n",
      "  Deleted file: assistant-Hdzq8njkrvA7BUxF9bihK1\n",
      "  Deleted file: assistant-FGxicRDzs3gr17NYTK6eMw\n",
      "  Deleted file: assistant-Wan2rEujCEr6JXW9xVuDpP\n",
      "  Deleted file: assistant-XrErxL8KBfUhoLRSHU8EN1\n",
      "  Deleted file: assistant-1E4DTsJnGTSH9Ehj6PGB3a\n",
      "  Deleted file: assistant-FsnTVerZJBv9XKGbhemehb\n",
      "  Deleted file: assistant-RgM3yoEzJ97CFzLfFodoZH\n",
      "Successfully deleted 170/170 files from Azure OpenAI\n",
      "Deleted vector store: vs_uB9u1bj3FblzSNsMxbUKzwEi\n"
     ]
    }
   ],
   "source": [
    "# Remove test files and vector store\n",
    "def cleanup():\n",
    "    print(\"Cleaning up...\")\n",
    "    \n",
    "    # Clean up local files\n",
    "    if temp_dir and os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Deleted test files directory: {temp_dir}\")\n",
    "    \n",
    "    # Clean up uploaded files from Azure OpenAI\n",
    "    print(\"Deleting uploaded files from Azure OpenAI...\")\n",
    "    all_uploaded_files = []\n",
    "    \n",
    "    # Collect all file IDs from different tests\n",
    "    if 'individual_file_ids' in globals():\n",
    "        all_uploaded_files.extend(individual_file_ids)\n",
    "    if 'concurrent_file_ids' in globals():\n",
    "        all_uploaded_files.extend([fid for fid in concurrent_file_ids if fid])\n",
    "    if 'stress_file_ids' in globals():\n",
    "        all_uploaded_files.extend([fid for fid in stress_file_ids if fid])\n",
    "    \n",
    "    try:\n",
    "        vs_files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "        batch_file_ids = [f.id for f in vs_files.data]\n",
    "        all_uploaded_files.extend(batch_file_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve vector store files for cleanup: {e}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_file_ids = list(set(all_uploaded_files))\n",
    "    print(f\"Found {len(unique_file_ids)} unique files to delete\")\n",
    "    \n",
    "    # Delete each file\n",
    "    deleted_count = 0\n",
    "    for file_id in unique_file_ids:\n",
    "        try:\n",
    "            client.files.delete(file_id)\n",
    "            deleted_count += 1\n",
    "            print(f\"  Deleted file: {file_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to delete file {file_id}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully deleted {deleted_count}/{len(unique_file_ids)} files from Azure OpenAI\")\n",
    "    \n",
    "    # Delete the vector store\n",
    "    try:\n",
    "        client.vector_stores.delete(vector_store_id)\n",
    "        print(f\"Deleted vector store: {vector_store_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete vector store: {e}\")\n",
    "\n",
    "cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
